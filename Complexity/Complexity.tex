\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection{Колмогоровская сложность моделей}
TODO: обозначения

Одним из фундаментальных способов определить сложность произвольного математического объекта является колмогоровская сложность. Ниже представлено формальеное определине колмогоровской сложности и основные ее соуйства.

\begin{definition}
Способом описания назовем вычислимое частинчо определенное отображение из множества бинарных слов в себя:
\[
D: \{0,1\}^{*}  \to  \{0,1\}^{*}.
\]
\end{definition}
% Определение
\begin{definition}
Пусть задан некоторый способ описания $D$. 
Колмогоровской сложностью бинарной строки $x$ назовем минимальную длину описания относительно $D$:
\[
K_D(x) = \min_{p \in \{0,1\}^*}\{|p|: D(p) = x\},
\]
\end{definition}


Перечислим некоторые свойства колмогоровской сложности~\cite{kolmogorov}.

\textbf{Независимости от способа написания.}
\begin{theorembd}
Пусть заданы  отображения $D_1$, $D_2$, такие что существуют константы $c_1, c_2$ такие что для любого другого отображения $D'$ и для любой строки $x$:
\[
	K_{D_1}(x) \leq K_{D'}(x) + c_1, \quad K_{D_2}(x) \leq K_{D'}(x) + c_2.
\]
Тогда $K_{D_1}(x) = K_{D_2}(x) + O(1).$
\end{theorembd}

Т.к. колмогоровская сложность независима от способа написания, зафиксируем некоторый способ описания $D$ и положим $K(x) = K_D(x).$

\textbf{Невычислимость }
\begin{theorembd}
Пусть $k$ --- произвольная вычислимая функция. Если $k(x) \leq K(x) $ для всех $x$, для которых определена $k$, то $k$ --- ограничена.
\end{theorembd}

Из теоремы следует, что колмогровская сложность в общем случае невычислима: любая оценка сложности будет ограничена, и потому тривиальна.

\textbf{Условная сложность}
Обобщим понятие колмогоровскорй сложности на случай двух бинарных строк.

\begin{definition}
Пусть задано вычисилмое и частично определенное отображение из декартового произведения двух множеств бинарных слов в себя:
\[
D: \{0,1\}^{*} \times  \{0,1\}^{*} \to  \{0,1\}^{*}.
\]

Условной колмогоровской сложностью бинарной строки $y$ при условии $x$ назовем минимальную длину описания относительно $D$:
\[
K_D(y|x) = \min_{p \in \{0,1\}^*}\{|p|: D(p, y) = x\},
\]
\end{definition}

% Chain Rule
\textbf{Оценка условной Колмогоровской сложности}~\cite{kolmogorov}
\[
	K(x,y) \leq K(x) + K(y|x) + O(\log K(x,y)).
\]


Разность $I(x:y) = K(y) - K(y|x)$ задает количество информации в $x$ об объекте $y$. 
\textbf{Количество информации в паре x,y симметрично с точностью до константы}:
\[
I(x:y) = I(y:x) + O(\text{log}K(x,y)).
\]

Отметим, что схожими свойствами обладает взаимная информация, определение которой дано ниже.

\begin{definition}
Пусть задана дискретная случайная величина $x$ с вероятностным распределением $p$, принимающая значения $x_1, \dots, x_n$,
Энтропией распределения случайной величины $x$ назовем:
\[
	H(x) = -\sum_{i=1}^n p(x = x_i) \log~p(x = x_i).
\]
Взаимной информацией $I$ двух случайных величин $x,y$ назовем следующее выражение:
\[
	I(x,y) = H(x) - H(x|y), \quad H(x) = - \sum_{i} p_x(x_i) \log p_x(x_i)ю
\]
\end{definition}


\[
	I(x,y) = I(y,x).
\]
Таким образом, свойства количества информации $I(x:y)$ и взаимной информации, во многом совпадают. Докажем теорему о связи колмогоровской сложности и энтропии распределения, подытоживающую связь этих двух математических объектов.

\begin{theorem}~\cite{grun_ks} 
Пусть задано семейство частично-определенных отображений $\mathfrak{D} = \{D: \{0,1\}^{*} \to \{0,1\}^{*}\}$,
такое что для любого отображения $D \in \mathfrak{D}$ и элемента из области определения $D$ в области определения не содержится префиксов этого элемента.
Пусть f --- вычислимая функция вероятности на пространстве бинарных векторов произвольной длины. Тогда 
\begin{equation}
\label{ks_entropy}
	0 \leq \bigl(\mathsf{E}_f K(X) - H(x) \bigr) \leq K(f)  + O(1).
\end{equation}
\end{theorem}
Для доказательства предварительно приведем две теоремы из~\cite{grun_ks} без доказательства.

\begin{theorembd}
\label{noiseless}
Пусть задано семейство частично-определенных отображений $\mathfrak{D} = \{D: \{0,1\}^{*} \to \{0,1\}^{*}\}$,
такое что для любого отображения $D \in \mathfrak{D}$ и элемента из области определения $D$ в области определения не содержится префиксов этого элемента.

Тогда для минимальной средней длины описания слова:
\[
	L = \min_{D \in \mathfrak{D}}\sum_{i}|D(x_i)|p(x = x_i)
\] 
справедливо неравенсвто:
\[
	H(x) \leq L \leq H(x)+1.
\]
\end{theorembd}

\begin{theorembd}
\label{kf_bounds}
Пусть $f$ --- вычислимое распределение на бинарных словах. Тогда справедлива следующие оценки:
\[
	2^{K(f) \pm O(1) - K(x)} \geq f(x),
\]
где $O(1)$ ---  длина некоторой программы, не зависящей от $f, x$. 
\end{theorembd}

Перейдем к доказательству основной теоремы.
\begin{proof}
Т.к. $K(X)$ --- это длина кода для $x$, то по теореме~\ref{noiseless}:
\[
	H(X) \leq L \leq \mathsf{E}_f K(X).
\]
Таким образом левая часть неравенства~\eqref{ks_entropy}  доказана.

По теореме~\ref{kf_bounds}:
\[
	f(x) \leq 2^{K(f) \pm O(1) - K(x)}.
\]
Тогда
\[
	 \log\frac{1}{f(x)} \geq K(f) - O(1) - K(x):
\]
Посчитаем матожидание данной величины по всем $x$:
\[
	 H(x) \geq \sum_x f(x) K(f) - \sum_x O(1) - \sum_x K(x).
\]
Пользуясь тем, что $\sum_{x} f(x) = 1$ получим итоговую формулу для правой части неравенства:
\[
	 H(x)  +   O(1) +  K(x) \geq \sum_x f(x) K(f),
\]
что и т.д.
\end{proof}




\subsection{Колмогоровская сложность и принцип минимальной длины описания}
Рассмотрим задачу выбора модели для заданной выборки. Будем полагать что заданная выборка описывается в виде некоторй бинарной строки $x$. В дальнейшем будем отождествлять выборки и ее бинарное описание $x$.

Для этого рассмотрим частный случай колмогоровской сложности, называемый префиксной колмогоровской сложностью. Эта сложность задается машиной Тьюринга специального вида, имеющей две ленты: однонаправленную ленту для чтения и двунаправленную рабочую ленту. Будем полагать что машина Тьюринга $T$ останавливается на $p$ с выводом $x$: $T(p) = x$, если вся запись $p$ осталась слева от читающей каретки, $x$ осталась слева от пишушщей каретки и $T$ остановлена.

\begin{definition}
Префиксная Колмогоровская сложность:
\[
K(x) = \min_{p \in \{0,1\}^* ,i \in \cN}\{|i|+|p|: T_i(p) = x\},
\]
где $|i|$ --- длина описания $i$-й префиксной машины Тьюринга.
\end{definition}

Задачу выбора модели для выборки можно рассматривать как задачу нахождения префиксной колмогоровской сложности для выборки. В случае, если модель является дискриминативной, то вместо колмогоровсокй сложности можно использовать условную колмогоровскую сложность.Т.к. колмогоровская сложность невычислима, рассмотрим упрощенный подход к выбору модели: вместо колмогоровской сложности строки $x$ будем искать некоторое множество $S$, в которое входит $x$, и чья сложность описания при помощи машины Тьюринга невелика. Таким образом, мы сможем найти ``хорошую'' машину Тьюрингу не для конкретной строки, а для некоторого семейства строк (или выборок), обладающих некоторыми общими свойствами или регулярностью.
\begin{definition}
Сложностью конечного множества $S$ назовем следующей величину:
\[
K(S) = \min_{p \in \{0,1\}^* ,i \in \cN}\{|i|+|p|: T_i(p) \text{ перечисляет все элементы множества } S\}.
\]
\end{definition}

Вместо задачи нахождения минимальной сложности для выборки $x$ будем искать множество $S$, которое описывается некоторой машиной Тьюринга, и в которое входит заданная строка $x$. Приведем формулу для оценки разности между сложность ю выборки $x$ и множества $S$, в которое входит данная выборка.
\begin{theorembd}
Для любого $x \in S$ справедливо неравенство~\cite{ks_struct}:
\[
	K(x) - K(S) \geq  + \log |S| + O(1).
\]
\end{theorembd}

На практике задача выбора модели подразумевает, что мы можем выбрать модель, которая описывает выборку (или множество выборок) $S$ неидеально, а с некоторым допустимым уровнем потери информации.
Тогда задача выбора модели для заданной выборки ставится следуюющим образом:
\begin{equation}
\label{eq:optim_ks_alpha}
	\argmin_{S} \{\log|S| + K(S): x \in S, K(s) \leq \alpha,
\end{equation}
где $\alpha$ --- максимально допустимая сложность множества $S$.

Заметим, что решение задачи выбора модели в приведенном выше виде является вычислимой, то есть можно предложить алгоритм, вычисляющий данную задачу. Приведем схему данного алгоритма:
\begin{enumerate}
\item Положим $\hat{p}, \hat{S}$ неопределенным.
\item Для всех $S, p: T(p) = S, |p| \leq \alpha$:
\item Если $\hat{S}$ неопределен или $|p| + \log(S) \leq \hat{p} + \log{\hat{S}},$ то $\hat{p}, \hat{S} = p, S$. 
\end{enumerate}
Т.к. множество всех программ с длиной менее $\alpha$ является конечным, то алгоритм остановится, а потому вычислим. По построению он также доставлячет решение оптимизационной задачи~\eqref{eq:optim_ks_alpha}.

\begin{theorembd}
Обзначим за $S^{*}, K(S) \leq \alpha$  множество, доставляющее минимум следующей функции:
\[
    \delta(S) = \log |S| - K(x|S).
\]
Тогда  $\delta(S^{*}) - \detla{\hat{S}} = O(\log |x|).$
\end{theorembd}

Таким образом, пара $(\hat{p}, \hat{S})$ доставляет минимуму суммы $|p| + \log(S)$ при ограничениях на длину программы $|p| \leq \alpha$. Первое слагаемое данной суммы --- это длина  программы, то есть сложность описания множества $\hat{S}$. Сама сумму $|p| + \log(S)$ характеризует длину кода, требуемого для двухэтапного описания строки $x$, где на первом этапе мы описываем множество $S$, а на втором этапе находим $x$ во множестве $S$.

Описанный метод получения кода для описания выборки $x$ соответствует \textit{Принципу миниммальной длины описания} или MDL: для заданной выборки требуется найти минимальный код, который описывает выборку (возможно, с некоторой наперед заданной допустимой ошибкой(.

\subsection{Вероятностная интерпретация минимальной длины описания}
С вероятностных позиций принцип минимальной длины описания формулируется следующим образом: задана выборка $X$, порожденная из некоторого вероятностного распределения $p$. Требуется построить код, доставляющий минимальную длину следующего выражения:
\[
    \log p(D|H) + L(H), 
\]
где $L(H)$ --- длина описания гипотезы $H$. 
Рассмотрим подробнее первый член данного слагаемого: $\log p(D|H)$. 
Определим понятие регрета:
\[
R(x) =  - \log P(x) + \min_{H} (\log P(x|H)).
\]
Регрет характеризует разницу между длиной истинного (?) кода для $x$ в сравнении с наилучшим кодом из $H$.

Пусть гипотеза $H$ --- зависит от параметра $w$:
\[
    \log p(D|H) = \log p(D|\hat{w}).    
\]
Тогда регрет выглядит следующим образом:
\[
R(x) =  - \log P(x) + \min_{w} (\log P(x|w)).
\]

Для всей выборки регрет определяется следующим образом:
\[
R(X) =  \max_{x} (- \log P(x) + \min_{w} (\log P(x|w))).
\]



\begin{thebibliography}{99}
\bibitem{kolmogorov}
	Успенский В., Шень А., Верещагин Н. Колмогоровская сложность и алгоритмическая случайность. – Litres, 2017

\bibitem{grun_ks}
Grunwald P., Vitányi P. Shannon information and Kolmogorov complexity //arXiv preprint cs/0410002. – 2004.

\bibitem{ks_struct}
Vereshchagin N. K., Vitányi P. M. B. Kolmogorov's structure functions and model selection //IEEE Transactions on Information Theory. – 2004. – Т. 50. – №. 12. – С. 3265-3290.
\end{thebibliography}

\end{document}


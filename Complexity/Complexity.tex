\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection{Колмогоровская сложность моделей}

Одним из фундаментальных способов определить сложность произвольного математического объекта является колмогоровская сложность. Ниже представлено формальное определение колмогоровской сложности и основные ее свойства.


\begin{definition}
Пусть задано вычислимое частично определенное отображение из множества бинарных слов в себя:
\[
T: \{0,1\}^{*}  \to  \{0,1\}^{*}.
\]
Колмогоровской сложностью бинарной строки $x$ назовем минимальную длину описания относительно $T$:
\[
K_T(x) = \min_{p \in \{0,1\}^*}\{|p|: T(p) = x\},
\]
\end{definition}
Заметим, что колмогоровская сложность зависит от отображения $T$. В~\cite{kolmogorov} доказано, что колмогоровсксая сложность $K_T(x)$ при двух отображениях $T_1, T_2$ отличается лишь на некоторую константу, не зависящих от строки $x$. Поэтому для дальнейшего изложения зафиксируем некоторое отображение $T$ и положим $K(x) = K_T(x)$.

TODO: Здесь рисунок + мотивация.

Обобщим понятие колмогоровскор сложности на случай двух бинарных строк.

\begin{definition}
Пусть задано вычислимое и частично определенное отображение из декартового произведения двух множеств бинарных слов в себя:
\[
T: \{0,1\}^{*} \times  \{0,1\}^{*} \to  \{0,1\}^{*}.
\]

Условной колмогоровской сложностью бинарной строки $y$ при условии $x$ назовем минимальную длину описания относительно $D$:
\[
K_T(y|x) = \min_{p \in \{0,1\}^*}\{|p|: T(p, y) = x\},
\]
\end{definition}
TODO: пояснить как связаны $K(x), K(y|x)$.

Рассмотрим некоторые свойства условной колмогоровской сложности.
% Chain Rule
\textbf{Оценка условной Колмогоровской сложности}~\cite{kolmogorov}
\[
	K(x,y) \leq K(x) + K(y|x) + O(\log K(x,y)).
\]


\textbf{Количество информации в паре x,y симметрично с точностью до константы}:
\[
I(x:y) = I(y:x) + O(\text{log}K(x,y)),
\]
где величина $I(x:y) = K(y) - K(y|x)$  задает количество информации в $x$ об объекте $y$. 

Отметим, что схожими свойствами обладает взаимная информация, определение которой дано ниже.

\begin{definition}
Пусть задана дискретная случайная величина $x$ с вероятностным распределением $p$, принимающая значения $x_1, \dots, x_n$,
Энтропией распределения случайной величины $x$ назовем:
\[
	H(x) = -\sum_{i=1}^n p(x = x_i) \log~p(x = x_i).
\]
Взаимной информацией $I$ двух случайных величин $x,y$ назовем следующее выражение:
\[
	I(x,y) = H(x) - H(x|y), \quad H(x) = - \sum_{i} p_x(x_i) \log p_x(x_i)ю
\]
\end{definition}


\[
	I(x,y) = I(y,x).
\]
Таким образом, свойства количества информации $I(x:y)$ и взаимной информации, во многом совпадают. Докажем теорему о связи колмогоровской сложности и энтропии распределения, подытоживающую связь этих двух математических объектов.

\begin{theorem}~\cite{kolmogorov}
Пусть задана некоторая строка $x$ длины $n$ с частотами  $p = (p_0, 1 - p_0)$ появлений нулей и единиц в строке.
Тогда
\[
K(x) \leq H(x) + O(\log m).
\]
Неравенство обращается в равенство для большинства строк $x$ длины $n$.
\end{theorem}
\begin{proof}
Всего слов, которые можно получить с использованием заданных частот:
\[
    C = \frac{m!}\frac{(p_0 m)! ((1-p_0)m)! }.
\]
Т.к. количество таких слов конечно, то их можно пронумеровать и построить отображение, выдающее строку $x$ по ее порядковому номеру.
Таким образом, условная колмогоровская сложность ограничена сверху:
\[
K(x|C, p_0) \leq \log C + O(1).
\]
Воспользуемся формулой Стирлинга:
\[
    n! = \sqrt{(2\pi + o(1))n}\frac{n}{e}^n.
\]
Отбрасывая полиномиальные множители (TODO) получим оценку:
\[
    C \leq 2^{nH(x)}.
\]

Для того, чтобы избавиться от условия в $K(x|C, p_0)$  потребуется $O(\log n)$ бит  (TODO) для описания чисел $p_0n, p_1n$, дающих в сумме $n$.

Поскольку слов с более короткими описаниями меньше, чем $C$, то для большинства слов будет достигаться предложенная оценка.
\end{proof}
\subsection{Колмогоровская сложность и принцип минимальной длины описания}
Рассмотрим задачу выбора модели для заданной выборки. Будем полагать что заданная выборка описывается в виде некоторй бинарной строки $x$. В дальнейшем будем отождествлять выборки и ее бинарное описание $x$.

Для этого рассмотрим частный случай колмогоровской сложности, называемый префиксной колмогоровской сложностью. Эта сложность задается машиной Тьюринга специального вида, имеющей две ленты: однонаправленную ленту для чтения и двунаправленную рабочую ленту. Будем полагать что машина Тьюринга $T$ останавливается на $p$ с выводом $x$: $T(p) = x$, если вся запись $p$ осталась слева от читающей каретки, $x$ осталась слева от пишушщей каретки и $T$ остановлена.

\begin{definition}
Префиксная Колмогоровская сложность:
\[
KP(x) = \min_{p \in \{0,1\}^* ,i \in \cN}\{|i|+|p|: T_i(p) = x\},
\]
где $|i|$ --- длина описания $i$-й префиксной машины Тьюринга.
\end{definition}

Отметим, что префиксная колмогоровская сложность является частным случаем колмогоровской сложности, а потому:
\[
K(x) \leq KP(x).
\]


Задачу выбора модели для выборки можно рассматривать как задачу нахождения префиксной колмогоровской сложности для выборки. В случае, если модель является дискриминативной, то вместо колмогоровсокй сложности можно использовать условную колмогоровскую сложность.Т.к. колмогоровская сложность невычислима, рассмотрим упрощенный подход к выбору модели: вместо колмогоровской сложности строки $x$ будем искать некоторое множество $S$, в которое входит $x$, и чья сложность описания при помощи машины Тьюринга невелика. Таким образом, мы сможем найти ``хорошую'' машину Тьюрингу не для конкретной строки, а для некоторого семейства строк (или выборок), обладающих некоторыми общими свойствами или регулярностью.
\begin{definition}
Сложностью конечного множества $S$ назовем следующей величину:
\[
K(S) = \min_{p \in \{0,1\}^* ,i \in \cN}\{|i|+|p|: T_i(p) \text{ перечисляет все элементы множества } S\}.
\]
\end{definition}

Вместо задачи нахождения минимальной сложности для выборки $x$ будем искать множество $S$, которое описывается некоторой машиной Тьюринга, и в которое входит заданная строка $x$. Приведем формулу для оценки разности между сложность ю выборки $x$ и множества $S$, в которое входит данная выборка.
\begin{theorembd}
Для любого $x \in S$ справедливо неравенство~\cite{ks_struct}:
\[
	K(x) - K(S) \geq  + \log |S| + O(1).
\]
\end{theorembd}

На практике задача выбора модели подразумевает, что мы можем выбрать модель, которая описывает выборку (или множество выборок) $S$ неидеально, а с некоторым допустимым уровнем потери информации.
Тогда задача выбора модели для заданной выборки ставится следуюющим образом:
\begin{equation}
\label{eq:optim_ks_alpha}
	\argmin_{S} \{\log|S| + K(S): x \in S, K(s) \leq \alpha,
\end{equation}
где $\alpha$ --- максимально допустимая сложность множества $S$.

Заметим, что решение задачи выбора модели в приведенном выше виде является вычислимой, то есть можно предложить алгоритм, вычисляющий данную задачу. Приведем схему данного алгоритма:
\begin{enumerate}
\item Положим $\hat{p}, \hat{S}$ неопределенным.
\item Для всех $S, p: T(p) = S, |p| \leq \alpha$:
\item Если $\hat{S}$ неопределен или $|p| + \log(S) \leq \hat{p} + \log{\hat{S}},$ то $\hat{p}, \hat{S} = p, S$. 
\end{enumerate}
Т.к. множество всех программ с длиной менее $\alpha$ является конечным, то алгоритм остановится, а потому вычислим. По построению он также доставлячет решение оптимизационной задачи~\eqref{eq:optim_ks_alpha}.

\begin{theorembd}
Обзначим за $S^{*}, K(S) \leq \alpha$  множество, доставляющее минимум следующей функции:
\[
    \delta(S) = \log{|S|} - K(x|S).
\]
Тогда  $\delta(S^{*}) - \delta{\hat{S}} = O(\log |x|).$
\end{theorembd}

Таким образом, пара $(\hat{p}, \hat{S})$ доставляет минимуму суммы $|p| + \log(S)$ при ограничениях на длину программы $|p| \leq \alpha$. Первое слагаемое данной суммы --- это длина  программы, то есть сложность описания множества $\hat{S}$. Сама сумму $|p| + \log(S)$ характеризует длину кода, требуемого для двухэтапного описания строки $x$, где на первом этапе мы описываем множество $S$, а на втором этапе находим $x$ во множестве $S$.

Описанный метод получения кода для описания выборки $x$ соответствует \textit{Принципу миниммальной длины описания} или MDL: для заданной выборки требуется найти минимальный код, который описывает выборку (возможно, с некоторой наперед заданной допустимой ошибкой(.

\subsection{Вероятностная интерпретация минимальной длины описания}
Рассмотрим подробнее задачу кодирования выборки. Задачу можно рассмаривать как проблему передачу информации от кодировщика декодировщику. Кодировщик кодирует информацию о выборке и передает ее декодировщику. Декодировщик раскодирует код, полученный от кодировщика и восстанавливает исходную выборку (возможно, с некоторой потерей информации, если это оговорено заранее).
Допустим, для кодирования информации о выборке доступно несколько методов кодирования, при этом для разных объектов выборки наилучший (т.е. минимальный по количеству информации) метод будет отличаться. Тогда кодировщик должен передать не только информацию о выборке, но и информацию о самом методе кодировния. Перейдем к вероятностной интерпретации минимальной длины описания.

\begin{theorembd}
Пусть задан конечное или счетное множество $Z$ с введенной на нем вероятностью $P$. Тогда существует префиксный код $C$, такой что для любого $x \in Z : L(C(x)) = -\log P(x)$.
\end{theorembd}

Таким образом, вместо методов кодирования монжо рассматривать задачу выбора функции вероятности, введенной на признаковом описании выборки. 
С вероятностных позиций принцип минимальной длины описания формулируется следующим образом: задана выборка $X$, требуется передать информацию о выборкем (с возможно, некоторой допустимой ошибкой) с использованием минимального количества информации. Пусть
 порожденная из некоторого вероятностного распределения $p$. Требуется построить код, доставляющий минимальную длину следующего выражения:
\[
    \log p(D|H) + L(H), 
\]
где $L(H)$ --- длина описания гипотезы $H$. 


Одним из критериев качества вероятностного кодирования является регрет:
\[
R(x) =  - \log P(x) + \min_{H} (\log P(x|H)).
\]
Регрет характеризует разницу между длиной предлагаеистинного кода для $x$ в сравнении с наилучшим кодом из некоторого множества $H$.

Пусть гипотеза $H$ --- зависит от параметра $w$:
\[
    \log p(D|H) = \log p(D|\hat{w}).    
\]
Тогда регрет выглядит следующим образом:
\[
R(x) =  - \log P(x) + \min_{w} (\log P(x|w)).
\]

Для всей выборки регрет определяется следующим образом:
\[
R(X) =  \max_{x} (- \log P(x) + \min_{w} (\log P(x|w))).
\]


\begin{theorem}[(Штарьков, 1987)~\ref{mdl,shtarkov}]
Пусть велеична
\[
    \log \sum_{x} p(x|w(x))
\]
конечна.  Тогда следующее выражени дает единственный минимум регрета:
\[
    \frac{p(x|w(x))}{\sum_{x}p(x|w(x)) }.
\]
\end{theorem}
\begin{proof}
Рассмотрим выражение регрета для данной вероятностной меры:
\[
    - \log P(x) + \min_{w} (\log P(x|w))  = \log \sum_{x} p(x|w(x)).
\]
Выражение регрета не зависит от объекта выборки $x$. 
Заметим, что для любых двух отличных распределений  $p_1$, $p_2$ существует хотя бы один элемент, такой что 
\[
    p_1(x) < p_2(x).
\]
Действительно, пусть это неверно: $\forall x p_1(x) \geq p_2(x), \exists x': p_1(x) > p_2(x)$. Просумсмируем вероятности $p_1$ и $p_2$ и воспользуемся тем, что сумма вероятноснетй по всем объектам будет равна единице:
\[
   1 =  \sum p_1  > \sum p_2 = 1,
\]
приходим к противоречию.

Тогда для любого распределения на выборке существует элемент $x'$:
\[
    \frac{p(x'|w(x'))}{\sum_{x}p(x|w(x)) } > p(x').
\]
Тогда 
\[
R(P, x)  \geq -\log p(x') + \log p(x'|w(x')) > \frac{p(x'|w(x'))}{\sum_{x}p(x|w(x)) } = R(x).
\]

\end{proof}

Байесовской интерпертацией минимальной длины описания выступает \textit{evidence} или \textit{обоснованность модели}:
\[
    \int_{w} p(x|w) p(w), 
\]
где $p(w)$ --- априорное распределения на выборке. оно задается на основе наших предположений о природе выборки и о способах ее порождения.

\begin{theorem}
Пусть $M$ --- экспоненциальное семейство. 

Пусть в качестве априорного распределения выступает распределение Джефри:
\[
    p(w) = \frac{\sqrt{I}}{\int_w \sqrt{I(w)}}, \text{ где } I --- \text{определить матрицы Фишера:}    
\]
\[
    I(w) = \text{det}\{\mathsf{E}_w \frac{-\partial^2}{\partial w_i \partial w_j } \log p(x|w) \}_{ij}.
\]
Тогда регрет можно аппроксимировать следующим выражением:
\[
\frac{k}{2}\log{\frac{n}{2\pi}} - \log p(x|w) + \log \sqrt{I(w)}.
\]

\end{theorem}

\begin{proof}
Воспользуемся аппроксимацией Лапласа для упрощения формулы обоснованности модели.
Разложим $\log p(x, w) = \log p(x|w)p(w)$ в точке локального максимума $w_0$ в ряд Тейлора:
\[
    \log p(x,w) \approx \log p(x,w_0)  - \frac{1}{2} (w-w_0)^\T A (w-w_0),
\]
\[
    A_{ij} = -\frac{\partial^2}{\partial w_i \partial w_j}\log p(x,w)|_{w = w_0}.
\]

Полученное распределение представимо в виде ненормированного гауссового распределения. для такой аппроксимации плотности вероятности запишем нормирующий коэффициент: 
\[
    \log p(x,w) \approx \log p(x|w_0) + p(w_0) - \log \sqrt{\frac{(2\pi)^k} - {\det A}}.
\]

Подставляя в полученную формулу распределение Джеффри получим:
\[
    p(x,w) \approx p(x| w_0) - \log\sqrt{\frac{(2\pi)^k}{2}}.
\]
TODO: там еще информация Фишера + экспоненциальное семейство.
Тогда регрет будет равен:
\[
    R(x)  \approx  \log\sqrt{\frac{(2\pi)^k}{2}}.
\]
\end{proof}

\begin{theorembd}
При количество параметров, стремящемся к бесконечности  оптимальной модели отличается от байесовской оценки на константу.
\end{theorembd}
\begin{thebibliography}{99}
\bibitem{kolmogorov}
	Успенский В., Шень А., Верещагин Н. Колмогоровская сложность и алгоритмическая случайность. – Litres, 2017

\bibitem{grun_ks}
Grunwald P., Vitányi P. Shannon information and Kolmogorov complexity //arXiv preprint cs/0410002. – 2004.

\bibitem{ks_struct}
Vereshchagin N. K., Vitányi P. M. B. Kolmogorov's structure functions and model selection //IEEE Transactions on Information Theory. – 2004. – Т. 50. – №. 12. – С. 3265-3290.

\bibitem{mdl}
Grunwald P. A tutorial introduction to the minimum description length principle //arXiv preprint math/0406077. – 2004.

\end{thebibliography}

\end{document}


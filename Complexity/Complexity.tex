\documentclass[../main.tex]{subfiles}
\begin{document}
\subsection{Колмогоровская сложность моделей}
TODO: обозначения

Одним из фундаментальных способов определить сложность произвольного математического объекта является колмогоровская сложность. Ниже представлено формальеное определине колмогоровской сложности и основные ее соуйства.

\begin{definition}
Способом описания назовем вычислимое частинчо определенное отображение из множества бинарных слов в себя:
\[
D: \{0,1\}^{*}  \to  \{0,1\}^{*}.
\]
\end{definition}
% Определение
\begin{definition}
Пусть задан некоторый способ описания $D$. 
Колмогоровской сложностью бинарной строки $x$ назовем минимальную длину описания относительно $D$:
\[
K_D(x) = \min_{p \in \{0,1\}^*}\{|p|: D(p) = x\},
\]
\end{definition}


Перечислим некоторые свойства колмогоровской сложности~\cite{kolmogorov}.

\textbf{Независимости от способа написания.}
\begin{theorembd}
Пусть заданы  отображения $D_1$, $D_2$, такие что существуют константы $c_1, c_2$ такие что для любого другого отображения $D'$ и для любой строки $x$:
\[
	K_{D_1}(x) \leq K_{D'}(x) + c_1, \quad K_{D_2}(x) \leq K_{D'}(x) + c_2.
\]
Тогда $K_{D_1}(x) = K_{D_2}(x) + O(1).$
\end{theorembd}

Т.к. колмогоровская сложность независима от способа написания, зафиксируем некоторый способ описания $D$ и положим $K(x) = K_D(x).$

\textbf{Невычислимость }
\begin{theorembd}
Пусть $k$ --- произвольная вычислимая функция. Если $k(x) \leq K(x) $ для всех $x$, для которых определена $k$, то $k$ --- ограничена.
\end{theorembd}

Из теоремы следует, что колмогровская сложность в общем случае невычислима: любая оценка сложности будет ограничена, и потому тривиальна.

\textbf{Условная сложность}
Обобщим понятие колмогоровскорй сложности на случай двух бинарных строк.

\begin{definition}
Пусть задано вычисилмое и частично определенное отображение из декартового произведения двух множеств бинарных слов в себя:
\[
D: \{0,1\}^{*} \times  \{0,1\}^{*} \to  \{0,1\}^{*}.
\]

Условной колмогоровской сложностью бинарной строки $y$ при условии $x$ назовем минимальную длину описания относительно $D$:
\[
K_D(y|x) = \min_{p \in \{0,1\}^*}\{|p|: D(p, y) = x\},
\]
\end{definition}

% Chain Rule
\textbf{Оценка условной Колмогоровской сложности}~\cite{kolmogorov}
\[
	K(x,y) \leq K(x) + K(y|x) + O(\log K(x,y)).
\]


Разность $I(x:y) = K(y) - K(y|x)$ задает количество информации в $x$ об объекте $y$. 
\textbf{Количество информации в паре x,y симметрично с точностью до константы}:
\[
I(x:y) = I(y:x) + O(\text{log}K(x,y)).
\]

Отметим, что схожими свойствами обладает взаимная информация, определение которой дано ниже.

\begin{definition}
Пусть задана дискретная случайная величина $x$ с вероятностным распределением $p$, принимающая значения $x_1, \dots, x_n$,
Энтропией распределения случайной величины $x$ назовем:
\[
	H(x) = -\sum_{i=1}^n p(x = x_i) \log~p(x = x_i).
\]
Взаимной информацией $I$ двух случайных величин $x,y$ назовем следующее выражение:
\[
	I(x,y) = H(x) - H(x|y), \quad H(x) = - \sum_{i} p_x(x_i) \log p_x(x_i)ю
\]
\end{definition}


\[
	I(x,y) = I(y,x).
\]
Таким образом, свойства количества информации $I(x:y)$ и взаимной информации, во многом совпадают. Докажем теорему о связи колмогоровской сложности и энтропии распределения, подытоживающую связь этих двух математических объектов.

\begin{theorem}~\cite{grun_ks} 
Пусть задано семейство частично-определенных отображений $\mathfrak{D} = \{D: \{0,1\}^{*} \to \{0,1\}^{*}\}$,
такое что для любого отображения $D \in \mathfrak{D}$ и элемента из области определения $D$ в области определения не содержится префиксов этого элемента.
Пусть f --- вычислимая функция вероятности на пространстве бинарных векторов произвольной длины. Тогда 
\begin{equation}
\label{ks_entropy}
	0 \leq \bigl(\mathsf{E}_f K(X) - H(x) \bigr) \leq K(f)  + O(1).
\end{equation}
\end{theorem}
Для доказательства предварительно приведем две теоремы из~\cite{grun_ks} без доказательства.

\begin{theorembd}
\label{noiseless}
Пусть задано семейство частично-определенных отображений $\mathfrak{D} = \{D: \{0,1\}^{*} \to \{0,1\}^{*}\}$,
такое что для любого отображения $D \in \mathfrak{D}$ и элемента из области определения $D$ в области определения не содержится префиксов этого элемента.

Тогда для минимальной средней длины описания слова:
\[
	L = \min_{D \in \mathfrak{D}}\sum_{i}|D(x_i)|p(x = x_i)
\] 
справедливо неравенсвто:
\[
	H(x) \leq L \leq H(x)+1.
\]
\end{theorembd}

\begin{theorembd}
\label{kf_bounds}
Пусть $f$ --- вычислимое распределение на бинарных словах. Тогда справедлива следующие оценки:
\[
	2^{K(f) \pm O(1) - K(x)} \geq f(x),
\]
где $O(1)$ ---  длина некоторой программы, не зависящей от $f, x$. 
\end{theorembd}

Перейдем к доказательству основной теоремы.
\begin{proof}
Т.к. $K(X)$ --- это длина кода для $x$, то по теореме~\ref{noiseless}:
\[
	H(X) \leq L \leq \mathsf{E}_f K(X).
\]
Таким образом левая часть неравенства~\eqref{ks_entropy}  доказана.

По теореме~\ref{kf_bounds}:
\[
	f(x) \leq 2^{K(f) \pm O(1) - K(x)}.
\]
Тогда
\[
	 \log\frac{1}{f(x)} \geq K(f) - O(1) - K(x):
\]
Посчитаем матожидание данной величины по всем $x$:
\[
	 H(x) \geq \sum_x f(x) K(f) - \sum_x O(1) - \sum_x K(x).
\]
Пользуясь тем, что $\sum_{x} f(x) = 1$ получим итоговую формулу для правой части неравенства:
\[
	 H(x)  +   O(1) +  K(x) \geq \sum_x f(x) K(f),
\]
что и т.д.
\end{proof}




\subsection{Колмогоровская сложность и принцип минимальной длины описания}
Рассмотрим частный случай колмогоровской сложности, называемый префиксной колмогоровской сложностью. Эта сложность задается машиной Тьюринга специального вида, имеющей две ленты: однонаправленную ленту для чтения и двунаправленную рабочую ленту. Будем полагать что машина Тьюринга $T$ останавливается на $p$ с выводом $x$: $T(p) = x$, если вся запись $p$ осталась слева от читающей каретки, $x$ осталась слева от пишушщей каретки и $T$ остановлена.

\begin{definition}
Префиксная Колмогоровская сложность:
\[
K(x) = \min_{p \in \{0,1\}^* ,i \in \cN}\{|i|+|p|: T_i(p) = x\},
\]
где $|i|$ --- длина описания $i$-й префиксной машины Тьюринга.
\end{definition}


\subsection{Вероятностная интерпретация минимальной длины описания}
% MDL: UML
% MDL: optimal
% MDL: Bayes




\begin{thebibliography}{99}
\bibitem{kolmogorov}
	Успенский В., Шень А., Верещагин Н. Колмогоровская сложность и алгоритмическая случайность. – Litres, 2017

\bibitem{grun_ks}
Grunwald P., Vitányi P. Shannon information and Kolmogorov complexity //arXiv preprint cs/0410002. – 2004.
\end{thebibliography}

\end{document}

